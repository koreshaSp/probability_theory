% 2023.05.15 lecture 11 (stepik, online)
\documentclass[../main.tex]{subfiles}
\begin{document}

\subsection{Примеры.}

\begin{exmpl}[матожидание и дисперсия суммы случайного числа случайных величин]
 Пусть случайные величины $ \xi_1, \xi_2, \xi_3, \ldots $ независимы, одинаково распределены, имеют матожидание $ \E\xi_n = a $ и дисперсию $ \D\xi_n = \sigma^{2} $. Пусть $ N $ --- дискретная случайная величина, принимающая целые неотрицательные значения. Рассмотрим случайную величину
 \begin{align*}
  S = \xi_1  + \xi_2 + \ldots + \xi_N
 \end{align*} --- это сумма случайного числа случайных величин. Чему равны $ \E S $ и $ \D S $ в терминах $ N $?

 \begin{itemize}
  \item Найдём сначала $ \E S $. По свойству \ref{umo_property_4} условного математического ожидания имеем
   \begin{align*}
    \E S &= \E(\E(S \mid N)) = \E \left( \sum_{n=0}^{\infty} \E(S \mid N = n) \cdot \Ind_{\left\{ N = n \right\}} \right) = \\
    &= \sum_{n=0}^{\infty} \E(S \mid N = n) \cdot P(N = n).
   \end{align*} Найдём условное математическое ожидание $ S $ относительно событий $ N = n $:
   \begin{align*}
    \E(S \mid N = n) = \E(\xi_1 + \ldots + \xi_n) = na.
   \end{align*} Подставим:
   \begin{align*}
    \E S = \sum_{n=0}^{\infty}na \cdot P(N = n) = a \sum_{n=0}^{\infty} n \cdot P(N = n) = a \cdot \E N.
   \end{align*} Ответ $ \E S = a\cdot \E N$ соответствует интуиции: в среднем будет $ \E N $ случайных величин, и у каждой матожидание равно $ a $. Более того, здесь мы не пользовались независимостью $ \xi_n $.
  \item Теперь найдём дисперсию $ \D S $. Так как мы уже знаем $ \E S $, достаточно найти $ \E S^{2} $. Поступим так же:
   \begin{align*}
    \E S^{2} = \E(\E(S^{2}\mid N)) = \sum_{n=0}^{\infty} \E(S^{2} \mid N = n) \cdot P(N = n).
   \end{align*} Найдём УМО $ S^{2} $ относительно событий $ N = n $:
   \begin{align*}
    \E(S^{2} \mid N = n) &= \E(\xi_1 + \xi_2 + \ldots + \xi_n)^{2} = \sum_{k=1}^{n} \E \xi_k^{2} + \sum_{k \neq j} \E\xi_k \cdot \E\xi_j,
   \end{align*} где в последнем переходе мы воспользовались независимостью $ \xi_k $ и $ \xi_j $. Так как $ \E\xi_k^{2} = \D\xi_k + (\E\xi_k)^{2} $, то
   \begin{align*}
    \E (S^{2} \mid N = n) &= \sum_{k=1}^{n} \D\xi_k + \sum_{k=1}^{n} (\E\xi_k)^{2} + \sum_{k \neq j} \E\xi_k \cdot \E\xi_j = \\
    &= n\sigma^{2} + na^{2} + n(n-1)a^{2} = n\sigma^{2} + n^{2}a^{2}.
   \end{align*} Подставим:
   \begin{align*}
    \E S^{2} &= \sum_{n=0}^{\infty}(n\sigma^{2} + n^{2}a^{2})\cdot P(N=n) = \\
    &= \sigma^{2} \sum_{n=0}^{\infty} n \cdot P(N=n) + a^{2} \sum_{n=0}^{\infty} n^{2}\cdot P(N=n) = \\
    &= \sigma^{2} \cdot \E N + a^{2}\cdot\E N^{2} = \sigma^{2}\cdot\E N + a^{2} \cdot \D N + a^{2}(\E N)^{2}.
   \end{align*} Наконец, выразим дисперсию $ S $:
   \begin{align*}
    \D S &= \E S^{2} - (\E S)^{2} = \sigma^{2} \cdot \E N + a^{2}\cdot\D N + a^{2}\cdot(\E N)^{2} - a^{2}\cdot(\E N)^{2} = \\
    &= \sigma^{2}\cdot\E N + a^{2} \cdot \D N.
   \end{align*}
 \end{itemize} Таким образом,
 \begin{align*}
  \E S = a \cdot \E N, && \D S = \sigma^{2} \cdot \E N + a^{2} \cdot \D N.
 \end{align*}
\end{exmpl}


\begin{exmpl}
 Пусть случайные величины $ \xi $ и $ \eta $ независимы и одинаково распределены. Чему равно $ \E(\xi \mid \xi + \eta) $?

 Докажем следующее важное равенство:
 \begin{align}
  \label{example:umo_of_two_nor_values}
  \E(\xi\mid\xi+\eta)=\E(\eta\mid\xi+\eta).
 \end{align}

 Обе эти случайные величины по построению измеримы относительно $ \sigma(\xi+\eta) $, поэтому для всякого измеримого множества $ A \in \sigma(\xi + \eta) $ необходимо проверить два равенства
 \begin{align*}
  \E(\E(\xi \mid \xi + \eta) \cdot \Ind_A) &= \E(\eta \cdot \Ind_A), \\
  \E(\E(\eta \mid \xi + \eta) \cdot \Ind_A) &= \E(\xi \cdot \Ind_A).
 \end{align*} Так как по определению мы имеем
 \begin{align*}
  \E(\E(\xi \mid \xi + \eta) \cdot \Ind_A) &= \E(\xi \cdot \Ind_A), \\
  \E(\E(\eta \mid \xi + \eta) \cdot \Ind_A) &= \E(\eta \cdot \Ind_A),
 \end{align*} то достаточно проверить
 \begin{align*}
  \E(\xi \cdot \Ind_A) = \E(\eta \cdot \Ind_A)
 \end{align*} для любого $ A \in \sigma(\xi + \eta) $. Всякое такое $ A $ имеет вид $ A = \left\{ \xi + \eta \in B \right\} $, $ B \in \B_1 $. Поэтому,
 \begin{align*}
  \E(\xi \cdot \Ind_A) &= \E(\xi \cdot \Ind_{\left\{ \xi+\eta \in B \right\}}) = \int_{\R^{2}} x \cdot \Ind_{\left\{ x + y \in B \right\}}(x, y)\,dP_{(\xi,\eta)} = \int_{\R^{2}} x \cdot \Ind_B(x + y)\,dP_{(\xi,\eta)}.
 \end{align*} Так как $ \xi $ и $ \eta $ независимы и одинаково распределены, то $ P_{(\xi,\eta)} = P \times P$, где $ P = P_\xi = P_\eta $. Тогда заменяя переменные получаем
 \begin{align*}
  \E(\xi \cdot \Ind_A) &= \int_{\R^{2}} x \cdot \Ind_B(x + y) \,dP \,dP = \int_{\R^{2}} y \cdot \Ind_B(x + y)  \,dP\,dP = \E(\eta \cdot \Ind_A).
 \end{align*} Равенство \eqref{example:umo_of_two_nor_values} проверено.

 Теперь воспользуемся линейностью условного математического ожидания (свойство \ref{umo_property_5}):
 \begin{align*}
  \E(\xi \mid\xi+\eta) = \frac{1}{2}\E(\xi\mid\xi+\eta) + \frac{1}{2}\E(\eta\mid\xi+\eta) = \frac{1}{2}\E(\xi + \eta \mid \xi + \eta) = \frac{\xi + \eta}{2}.
 \end{align*}
\end{exmpl}

\begin{exmpl}[производящая функция случайного числа случайных величин]
 \label{example:genfun_of_random_number_of_random_values}
 Пусть случайные величины $ \xi_1, \xi_2, \ldots $  независимы, одинаково распределены и принимают целые неотрицательные значения. Пусть $ F $ --- их производящая функция.

 Пусть  $ N $  --- случайная величина, принимающая целые неотрицательные значения, и пусть $ G $  --- производящая функция $ N $. Обозначим
 \begin{align*}
  S = \xi_1 + \xi_2 + \ldots + \xi_N.
 \end{align*}

 Найдём производящую функцию $ H $ случайной величины $ S $:
 \begin{align*}
  H(z) &= \E z^{S} = \E(\E(z^{S} \mid N)) = \sum_{n=0}^{\infty} P(N=n) \cdot \E(z^{S} \mid N = n) = \\
  &= \sum_{n=0}^{\infty}P(N=n) \cdot \E z^{\xi_1+\ldots+\xi_n} = \sum_{n=0}^{\infty} P(N=n) \cdot \E z^{\xi_1} \cdot \ldots \cdot \E z^{\xi_n} = \\
  &= \sum_{n=0}^{\infty} P(N=n) \cdot (F(z))^{n} = G(F(z)).
 \end{align*} Получили красивую формулу $ H(z) = G(F(z)) $.
\end{exmpl}

\newpage
\section{Ветвящийся процесс.}

\subsection{Ветвящийся процесс.}

Рассмотрим первый пример дискретного случайного процесса, который называется \textit{ветвящийся процесс}.

Предположим, у нас есть множество неразличимых частиц. На каждом шаге каждая частица может разделится на несколько частиц: с вероятностью $ f_k $,  $ k \geqslant 0 $ каждая частица превращается в $ k $  частиц.  В частности, частица может исчезнуть с вероятностью $ f_0 $. Разумеется, от чисел $ f_k $ мы требуем
\begin{align*}
 \sum_{k=0}^{\infty}f_k = 1, \qquad f_k \geqslant 0.
\end{align*}

Пускай в начальный момент времени у нас есть одна частица. На первом шаге она делится на сколько-то частиц. Те, в свою очередь, на втором шаге делятся ещё на несколько частиц и так далее.

Обозначим за случайную величину $ \eta_n $ количество частиц после $ n $ шагов ($ \eta_0 = 1 $). Так,
\begin{align}
 \label{eq:eta_branching_process}
 \eta_{n+1} = \xi^{(n+1)}_{1} + \xi^{(n+1)}_2 + \ldots \xi^{(n+1)}_{\eta_n},
\end{align} где случайные величины $ \xi^{(n+1)}_k $ обозначают количество частиц, в которые превратилась $ k $-я частица (из $ \eta_n $ частиц) на шаге $ n+1 $. Величины $ \xi_k^{(n+1)} $ независимы и одинаково распределены дискретным образом с вероятностями $ P(\xi^{(n)}_k = m) = f_m $.

Обозначим за $ G_n(z) $ производящую функцию для $ \eta_n $. Попробуем найти её. Так как формула \eqref{eq:eta_branching_process} представляет из себя сумму случайного числа случайных величин (см. пример \ref{example:genfun_of_random_number_of_random_values}), то
\begin{align*}
 G_{n+1}(z) = G_n(G(z)),
 \end{align*} где \begin{align*}
 G(z) = \sum_{k=0}^{\infty}f_kz^{k}
\end{align*} --- производящая функция случайных величин $ \xi^{(n)}_k $ (и по совместительству $ G(z) = G_1(z) $ --- производящая функция величины $ \eta_1 $). При этом,
\begin{align*}
 G_0(z) = z,
\end{align*} так как $ \eta_0 = 1 $. По индукции получаем равенство
\begin{align*}
 G_n(z) = \underbrace{G(G(\ldots}_{n\text{ раз}}(z)\ldots)).
\end{align*}

Обозначим через $ m = \E \xi_1^{(1)} = G'(1) $ математическое ожидание количества частиц, получаемых за один шаг из одной частицы. Зная производящую функцию, мы сразу же можем вычислить математическое ожидание количества частиц после $ n $ шагов:
\begin{align*}
 \E \eta_n &= G_n'(1) = \left.(G_{n-1}(G(z)))'\right|_{z=1} = \\
  &= G_{n-1}'(\underbrace{G(1)}_{1}) \cdot G'(1) = G_{n-1}'(1) \cdot m = \E\eta_{n-1} \cdot m,
 \end{align*} а также $ \E \eta_0 = G_0'(1) = 1 $. По индукции получаем
 \begin{align*}
  \E\eta_n = m^{n}.
 \end{align*}

 \subsection{Вероятность вырождения ветвящегося процесса.}

 \begin{df}
  Будем говорить, что ветвящийся процесс \textit{выродился}, если в какой-то момент не осталось частиц (оказалось $ \eta_n = 0 $ для некоторого $ n $). Соответственно, \textit{вероятностью вырождения} ветвящегося процесса назовём вероятность того, что $ \eta_n = 0 $ для некоторого $ n $.
 \end{df}

 \begin{thm}
  \label{theorem:stop_branching_process}
  Вероятность вырождения ветвящегося процесса равна наименьшему корня уравнения $ G(x) = x $ на отрезке $ [0,1] $.
 \end{thm}

 Заметим что, хотя бы один корень на отрезке $ [0,1] $ всегда есть, так как $ G(1) = 1 $. Кроме того, среди всех корней есть наименьший по непрерывности функции $ G(x)-x $.

 \begin{proof}[\normalfont\textsc{Доказательство теоремы \ref{theorem:stop_branching_process}}]
  Обозначим событие $ A_n = \left\{ \eta_n = 0 \right\} $. Тогда
  \begin{align*}
   P(A_n) = G_n(0)
  \end{align*} --- свободный член ряда $ G_n(z) $. Отметим также, что события вложены:
  \begin{align*}
   A_n \subset A_{n+1}
  \end{align*}
  (ведь $ \eta_n = 0 $ влечёт $ \eta_{n+1} = 0 $), поэтому вероятности этих событий растут:
  \begin{align*}
   P(A_n) \leqslant P(A_{n+1}).
  \end{align*} Так как вероятности ограничены единицей, то существует предел
  \begin{align*}
   q = \lim_{n \to \infty} P(A_n) = \lim_{n \to \infty} G_n(0) \in [0,1].
  \end{align*} Так как события $ A_n $ вложены, то по непрерывности меры сверху
  \begin{align*}
   q = P \left( \bigcup_{n=0}^{\infty} A_n \right),
  \end{align*} то есть $ q $ в точности равно вероятности вырождения ветвящегося процесса.

  Давайте сначала докажем, что $ q $ есть решение уравнения $ G(x) = x $, для этого вычислим $ G(q) $:
  \begin{align*}
   G(q)&=G \left( \lim_{n \to \infty} G_n(0) \right) = \lim_{n \to \infty} G(G_n(0)) = \lim_{n \to \infty} G_{n+1}(0) = q.
  \end{align*} Здесь мы пользовались тем, что производящая функция $ G $ непрерывна.

  Осталось доказать, что $ q $ обязан быть наименьшим корнем. Для этого сначала поймём, что функция $ G $ обязана возрастать на отрезке $ [0,1] $. Действительно, её производная неотрицательна на $ [0,1] $:
  \begin{align*}
   G'(x) = \sum_{k=0}^{\infty} k f_k x^{k-1} \geqslant 0.
  \end{align*} Пусть теперь $ y $ --- наименьший корень уравнения $ G(x) = x $ на $ [0,1] $. Докажем по индукции, что $ G_n(0) \leqslant y $ при всех $ n $. 
  \begin{itemize}
   \item База $ n=0 $: $ G_0(0) = 0 \leqslant y $.
   \item Переход $ n \mapsto n+1 $. Если $ G_n(0) \leqslant y $, то по возрастанию $ G $ имеем
    \begin{align*}
     G_{n+1}(0) = G(G_n(0)) \leqslant G(y) = y.
    \end{align*}
  \end{itemize} В таком случае,
  \begin{align*}
   q = \lim_{n \to \infty} G_n(0) \leqslant y,
  \end{align*} что и требовалось доказать.
 \end{proof}

 \begin{remrk*}

  Обсудим получившийся результат. В процессе доказательства теоремы \ref{theorem:stop_branching_process} мы показали
  \begin{align*}
   G'(x) = \sum_{k=0}^{\infty} k f_k x^{k-1} \geqslant 0 \implies G(x) \text{ возрастает при } x \geqslant 0.
  \end{align*} Можно продифференцировать ещё раз, и тем самым показать
  \begin{align*}
   G''(x) = \sum_{k=0}^{\infty} k (k-1) f_k x^{k-2} \geqslant 0 \implies G(x) \text{ выпуклая при } x \geqslant 0.
  \end{align*}

  Обозначим за $ m = \E \xi_1^{(1)} $ математическое ожидание количества частиц, получаемых за один шаг из одной частицы. По совместительству $ m = G'(1) $ --- угловой коэффициент касательной к графику функции $ G(x) $ в точке $ x = 1 $.

  Тогда если $ m < 1 $, то из-за выпуклости функции $ G(x) $, весь график $ G(x) $ лежит строго над прямой $ y = x $ на отрезке $ [0,1] $ (за исключением одной точки $ (1,1) $). В этой ситуации заведомо есть лишь один корень уравнения $ G(x) = x $ на отрезке $ [0,1] $, и это $ x = 1 $. Таким образом, при $ m < 1 $ вероятность вырождения ветвящегося процесса равна $ 1 $, то есть ветвящийся процесс выродится почти наверное.

  \begin{figure}[ht]
   \centering
   \incfig{g_convex}
   \caption{Возможные случаи графика $ G(x) $ относительно $ m=G'(1) $.}
   \label{fig:g_convex}
  \end{figure}

  Если же $ m > 1 $, то в силу выпуклости прямая пересечёт график в двух точках, одна из них --- это $ x = 1 $, а вторая лежит в полуинтервале $ [0,1) $. 

  Вторая точка пересечения равна $ (0,0) $, если $ G(0) = 0 $, а это верно, если и только если $ f_0 = 0 $. Таким образом, вероятность вырождения ветвящегося процесса равна нулю, тогда и только тогда, когда никакая частица не может исчезнуть. Если же $ f_0 > 0 $, то вероятность вырождения положительна (оно и понятно: уже первая частица может исчезнуть с ненулевой вероятностью).

  В критическом случае $ m = 1 $ могут быть два разных варианта. Первый: график $ G(x) $ всё так же лежит строго над прямой $ y = x $, и тогда вероятность вырождения равна $ 1 $, как и в случае $ m < 1 $. Второй: график $ G(x) $ пересекает прямую $ y = x $ во второй точке $ u \in [0, 1) $. Но тогда $ G(v) = v $ при всех $ v \in [u, 1] $, поэтому производящая функция просто равна $ G(z)=z $ (ведь аналитические функции совпадают на отрезке, из чего следует, что они совпадают везде). Этот случай соответствует распределению $ f_1 = 1 $ --- когда частица так и будет оставаться одной. В этом случае вероятность вырождения нулевая.

 \end{remrk*}

 \subsection{Скорость вырождения в критическом случае.}

 Исследуем подробнее ветвящийся процесс с $ m = 1 $. Мы знаем, что такой процесс (за исключением случая $ G(x) = x $, когда $ f_1 = 1 $) вырождается с вероятностью $ 1 $. Обсудим скорость вырождения такого ветвящегося процесса.

 \begin{thm}
  Пусть
  \begin{align*}
   m = \E \xi_1^{(1)} = G'(1) = 1, && b = \D \xi_1^{(1)} = G''(1) \in (0, +\infty)
  \end{align*}
  --- математическое ожидание и дисперсия количества частиц, получающихся за один шаг из одной частицы в ветвящемся процессе. Обозначим за $ q_n $ вероятность того, что после шага $ n $ нет частиц (то есть $ \eta_n=0 $), а за $ \gamma_n $ обозначим вероятность вырождения на шаге $ n $ (то есть после шага $ n-1 $ ещё были частицы, а после шага $ n $ их не стало). Тогда
  \begin{align}
   \label{eq1:thm:speed_of_branching_process} p_n = 1 - q_n \sim \frac{2}{bn}, \\
   \label{eq2:thm:speed_of_branching_process} \gamma_n \sim \frac{2}{bn^{2}}.
  \end{align}
 \end{thm}
 \begin{proof}[\normalfont\textsc{Доказательство}]
  Заведём вспомогательную функцию
  \begin{align*}
   g(x) = 1 - G(1 - x), \qquad x \in [0,1].
  \end{align*} Заметим, что
  \begin{align*}
   q_n = G_n(0) = G(G_{n-1}(0)) = G(q_{n-1}), \qquad q_0 = 0.
  \end{align*} Тогда
  \begin{align*}
   p_n = g(p_{n-1}), \qquad p_0 = 1.
  \end{align*} Отметим, что поскольку $ q_n $ стремится к $ 1 $ (как к вероятности вырождения процесса), то $ p_n \to 0 $.

  В свою очередь,
  \begin{align*}
   \gamma_n = q_{n} - q_{n-1} = p_{n-1} - p_n.
  \end{align*}

  Запишем теперь формулу Тейлора для функции $ g(x) $ при $ x \to 0 $. Во-первых,
  \begin{align*}
   g(0) = 1 - G(1 - 0) = 1 - 1 = 0.
   \end{align*} Во-вторых,\begin{align*}
   g'(x) = G'(1-x),
   \end{align*} и \begin{align*}
   g'(0) = G'(1) = m = 1.
  \end{align*} В-третьих,
  \begin{align*}
   g''(x) = -G''(1-x),
   \end{align*} и \begin{align*}
   g''(0) = -G''(1) = -b.
  \end{align*} Таким образом, при $ x \to 0 $ верно
  \begin{align}
   \label{eq:taylor_g:speed_of_branching_process}
   g(x) = x - \frac{bx^{2}}{2} + o(x^{2}).
  \end{align} Исходя из формулы \eqref{eq:taylor_g:speed_of_branching_process} будем смотреть на асимптотику $ p_n $ и $ \gamma_n $.

  Рассмотрим величины $ a_n = 1 / p_n $. Учитывая $ p_n \to 0 $, запишем разность соседних членов:
  \begin{align*}
   a_{n+1} - a_n &= \frac{1}{p_{n+1}} - \frac{1}{p_n} = \frac{p_n - p_{n+1}}{p_n p_{n+1}} = \frac{p_n - g(p_n)}{p_n \cdot g(p_n)} = \\
   &= \frac{p_n -p_n + \frac{bp_n^{2}}{2} + o(p_n^{2})}{p_n \cdot \left( p_n + o(p_n) \right)} = \frac{b / 2 + o(1)}{1 + o(1)} \to \frac{b}{2}.
  \end{align*} По теореме Штольца
  \begin{align*}
   \frac{a_n}{n} \to \frac{b}{2} \implies p_n \sim \frac{2}{bn}.
  \end{align*}

  Формула \eqref{eq1:thm:speed_of_branching_process} доказана. А формула \eqref{eq2:thm:speed_of_branching_process} просто следует из неё:
  \begin{align*}
   \gamma_n = p_{n-1} - p_{n} \sim \frac{2}{b(n-1)} - \frac{2}{bn} = \frac{2(n - (n-1))}{b(n-1)n} \sim \frac{2}{bn^{2}}.
  \end{align*}
 \end{proof}

 \end{document}
