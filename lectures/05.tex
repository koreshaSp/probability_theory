% 2023.03.14: lecture 05
\documentclass[../main.tex]{subfiles}
\begin{document}

\newpage
\section{Математическое ожидание и дисперсия.}

В этом параграфе мы определим ключевые характеристики случайных величин: математическое ожидание, дисперсию, моменты и другие.

\begin{df}[математическое ожидание]
 \textit{Математическим ожиданием} (или \textit{матожиданием}) cлучайной величины $ \xi \colon \, \Omega \to \R $ называется число
 \begin{align}
  \label{equation:expected_value_def}
  \E \xi = \int\limits_{\Omega}  \xi\,dP,
 \end{align} где интеграл берётся по мере $ P $ --- вероятности.
\end{df}
\begin{remrk*}
 В общем случае у случайной величины может не быть математического ожидания! Такое может быть, когда интеграл \eqref{equation:expected_value_def} не существует (расходится). Например, распределение Коши.

 Если $ \xi \geqslant 0 $ с вероятностью $ 1 $, то можно считать, что $ \E \xi $ существует всегда, но тогда возможен случай $ \E \xi = \infty $ (интеграл расходится).
\end{remrk*}
В следующем предложении перечислим свойства матожидания, непосредственно следующие из свойств интеграла Лебега.
\begin{prop}\
 \begin{enumerate}
  \item Математическое ожидание линейно:
   \begin{align*}
    \E(a\xi + b\eta) = a \E \xi + b \E \eta.
   \end{align*} Более формально, левая часть определена тогда и только тогда, когда определены оба слагаемых в правой части, и в этом случае выполнено равенство.
  \item Если $ \xi \geqslant 0 $ с вероятностью $ 1 $, то $ \E\xi\geqslant 0 $.
  \item Если $ \xi \geqslant \eta $ с вероятностью $ 1 $, то $ \E\xi \geqslant \E\eta $.
 \end{enumerate}
\end{prop}

Важное замечание: линейность матожидания верна даже если величины $ \xi $ и $ \eta $ не являются независимыми. Это крайне важное и часто применимое свойство.

Математическое ожидание можно выразить вообще не привлекая вероятностное пространство, исключительно в терминах распределения.
\begin{prop}
 Если математическое ожидание cлучайной величины $ \xi $ существует, то верна формула
 \begin{align}
  \label{equation:expected_value_integral_on_real}
  \E\xi = \int\limits_{\R} x\,dP_{\xi}(x),
 \end{align} где $ P_\xi $ --- распределение случайной величины $ \xi $ как борелевская мера на $ \R $.
\end{prop}
Формула \eqref{equation:expected_value_integral_on_real} будет частным случаем следующей полезной формулы.
\begin{prop}
 Если $ \xi_1, \xi_2, \ldots, \xi_n \colon\, \Omega \to \R $ --- случайные величины, и $ f \colon\, \R^{n}\to\R $ --- измеримая по Борелю функция, то
 \begin{align}
  \label{equation:expected_value_of_function}
  \E f(\xi_1, \xi_2, \ldots, \xi_n) = \int\limits_{\R^{n}} f(x_1, \ldots, x_n) \,d P_{\vec\xi}(x_1, \ldots, x_n),
 \end{align} где $ P_{\vec\xi} $ --- совместное распределение случайных величин $ \vec\xi = (\xi_1, \xi_2, \ldots, \xi_n) $ как борелевская мера на $ \R^{n} $.
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Доказательство будет по стандартной схеме из теории меры.
 \begin{itemize}
  \item Пусть $ f = \Ind_A $, $ A \subset \R^{n} $ --- борелевское множество. Тогда
   \begin{align*}
    \E f (\xi_1, \ldots, \xi_n) = P(\vec\xi \in A) = P_{\vec\xi}(A) = \int\limits_{\R^{n}} \Ind_A\,dP_{\vec\xi}.
   \end{align*}  Для индикаторных функций формула \eqref{equation:expected_value_of_function} доказана.
  \item По линейности матожидания и интеграла Лебега формула \eqref{equation:expected_value_of_function} доказана для простых неотрицательных функций.
  \item Пусть неотрицательная функция $ f \geqslant 0 $ измерима. По теореме об аппроксимации существуют простые неотрицательные функции $ f_1 \leqslant f_2 \leqslant \ldots $ такие, что $ f_k \to f $ всюду. По предыдущему пункту для каждого $ k \geqslant 1 $ имеем \begin{align*}
    \E f_k(\xi_1, \xi_2, \ldots, \xi_n) = \int\limits_{\R^{n}} f_k(\vec x) \,dP_{\vec\xi}(\vec x).
  \end{align*} По теореме Леви правая часть стремится к
   \begin{align*}
    \lim_{k \to \infty} \int\limits_{\R^{n}} f_k(\vec x)  \,dP_{\vec\xi}(\vec x) = \int\limits_{\R^{n}} f(\vec x)\,dP_{\vec\xi} (\vec x),
   \end{align*} а левая часть к
   \begin{align*}
    \lim_{k \to \infty} \E f_k (\xi_1, \ldots, \xi_n) &= \lim_{k \to \infty} \int\limits_{\Omega} f_k(\xi_1, \ldots, \xi_n) \,dP = \int\limits_{\Omega} f(\xi_1, \ldots, \xi_n)\,dP = \\
    &= \E f(\xi_1, \ldots, \xi_n).
   \end{align*} Равенство \eqref{equation:expected_value_of_function} доказано для неотрицательных измеримых функций.
  \item Для функций произвольного знака рассмотреть $ f_\pm = \max(\pm f, 0) $ и воспользоваться линейностью.
 \end{itemize}
\end{proof}
\begin{crly}
 Математическое ожидание зависит только от распределения случайной величины.
\end{crly}

Следующее свойство матожидания не имеет никакого аналога в теории меры, оно имеет смысл лишь в теории вероятностей.

\begin{prop}
 \label{proposition:independent_expected_product}
 Если случайные величины $ \xi $ и $ \eta $ независимы, то
 \begin{align}
  \label{equation:independednt_expected_product}
  \E(\xi \eta) = \E\xi \cdot \E\eta.
 \end{align} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Воспользуемся формулой \eqref{equation:expected_value_of_function} для функции $ f(x,y) = xy $; тем фактом, что совместное распределение $ \xi $ и $ \eta $ является произведением распределений (теорема \ref{theorem:random_variable_independence_through_distribution_measure_eq}), а также теоремой Фубини:
 \begin{align*}
  \E(\xi\eta) &= \int\limits_{\R^{2}} xy \,dP_{(\xi,\eta)} (x,y) = \int\limits_{\R^{2}} xy\,d(P_{\xi} \times P_{\eta})(x,y) = \int\limits_{\R} \left( \int\limits_{\R} xy\,dP_\eta(y)  \right) dP_\xi(x) = \\
  &=  \int\limits_{\R} x\,dP_\xi(x)  \cdot  \int\limits_{\R} y\,dP_\eta(y)   = \E \xi \cdot \E\eta.
 \end{align*} 
\end{proof}
\begin{crly}
 Если cлучайные величины $ \xi_1,\xi_2, \ldots, \xi_n $ независимы, то
 \begin{align*}
  \E(\xi_1 \xi_2 \ldots \xi_n) = \E\xi_1 \cdot \E\xi_2 \cdot \ldots \cdot \E\xi_n.
 \end{align*} 
\end{crly}
\begin{remrk}
 Формула \eqref{equation:independednt_expected_product} не верна без независимости случайных величин $ \xi $ и $ \eta $.
\end{remrk}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Пусть $ \xi, \eta $ равны $ \pm 1 $ с вероятностью $ \frac{1}{2} $. Тогда $ \E\xi = \E\eta = 0 $. Но если $ \xi = \eta $, то $ \E(\xi\eta) = \E(1) = 1 $!
\end{proof}
\begin{remrk}
 Из равенства \eqref{equation:independednt_expected_product} не следует независимость случайных величин $ \xi $ и $ \eta $.
\end{remrk}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Пусть пространство элементарных исходов состоит из четырёх исходов: $ \Omega = \left\{ \omega_1, \omega_2, \omega_3, \omega_4 \right\} $ с одинаковыми вероятностями, а случайные величины $ \xi $ и $ \eta $ принимают следующие значения:
 \begin{center}
  \begin{tabular}{r|cccc}
   & $\omega_1$ & $\omega_2$ & $\omega_3$ & $\omega_4$ \\
   \hline
   $\xi$ & $ 1 $ & $0$ & $0$ & $-1$ \\
   $\eta$ & $0$ & $1$ & $-1$ & $0$
  \end{tabular}
 \end{center}
 Тогда  $ \E \xi = \frac{1 + 0 + 0 + (-1)}{4} = 0 $, $ \E\eta = \frac{0 + 1 + (-1) + 0}{4} = 0 $, $ \E (\xi \eta) = \frac{0 + 0 + 0 + 0}{4} = 0 $, и равенство \eqref{equation:independednt_expected_product} выполнено. Однако независимости нет: например события $ \left\{ \xi=1 \right\} $ и $ \left\{ \eta = 0 \right\} $ зависимы.
\end{proof}

\begin{prop}
 Если $ \xi \geqslant 0 $ почти всюду, то
 \begin{align*}
  \E\xi = \int\limits_{0}^{+\infty} P(\xi \geqslant t)\,dt.
 \end{align*} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Применим теорему Тонелли:
 \begin{align*}
  \E\xi &= \int\limits_{\Omega} \xi(\omega)\,dP(\omega) = \int\limits_{\Omega} \int\limits_{0}^{\xi(\omega)} dt\,dP(\omega) = \int\limits_{0}^{+\infty} \int\limits_{\Omega}   \Ind_{[0, \xi(\omega)]}(t)\,dP(\omega)\,dt = \\
  &= \int\limits_{0}^{+\infty} P(\xi(\omega) \geqslant t)\,dt.
 \end{align*} 
\end{proof}

Отметим некоторые неравенства с матожиданием.

\begin{prop}[неравенство Гёльдера]
 \label{proposition:gulder_inequality}
 Если числа $ p, q > 1 $ таковы, что $ 1/p+1/q = 1 $, то
 \begin{align}
  \label{equation:holder_inequality}
  \E\left|\xi\eta\right| \leqslant \left( \E \left| \xi \right|^{p} \right)^{1/p} \left( \E \left| \eta \right|^{q} \right)^{1/q}.
 \end{align} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Применим неравенство Гёльдера для интегралов:
 \begin{align*}
  \int_{\Omega} \left|\xi \eta \right|\,dP \leqslant \left( \int_{\Omega} \left| \xi \right|^{p}  \right)^{1 / p} \left( \int_{\Omega} \left| \eta \right|^{q}  \right)^{1 / q}.
 \end{align*}
\end{proof}

\begin{prop}[неравенство Ляпунова]
 \label{proposition:Lyapunov_inequality}
 Если $ 0 < r < s $, то 
 \begin{align*}
  \left(\E \left| \xi \right|^{r} \right)^{1/r} \leqslant \left( \E \left| \xi \right|^{s} \right)^{1/s}.
 \end{align*} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Возведём обе части в степень $r$. Нужно проверить 
 \begin{align*}
  \E \left|\xi\right|^r \leqslant (\E \left|\xi\right|^s)^{r/s}
 \end{align*}

 Воспользуемся неравенством Гёльдера \eqref{equation:holder_inequality}, взяв $ \xi = \left| \xi \right|^{r} $, $ \eta = 1 $, $ p = s / r > 1 $ и соответствующее $ q $:
 \begin{align*}
  \E \left| \xi \right|^{r} \leqslant \left( \E (\left| \xi \right|^{r})^{s / r} \right)^{r / s} \cdot \left( \E 1^{q} \right)^{1 / q} = \left( \E \left| \xi \right|^{s} \right)^{r / s}.
 \end{align*}
\end{proof}

\begin{prop}[неравенство Маркова]
 Пусть $ \xi \geqslant 0 $ почти всюду и число $ t > 0 $. Тогда
 \begin{align}
  \label{equation:markov_inequality}
  P(\xi \geqslant t) \leqslant \frac{\E \xi}{t}.
 \end{align}  
\end{prop}

Неравенство \eqref{equation:markov_inequality} Маркова было в теории меры (там оно называлось неравенством Чебышева; в теории вероятностей такое название будет у другого неравенства).

\begin{proof}
 \begin{align*}
  \E \xi = \int\limits_{\Omega} \xi\,dP \geqslant \int\limits_{\left\{ \xi \geqslant t \right\}} \xi \,dP \geqslant \int\limits_{\left\{ \xi \geqslant t \right\}}  t\,dP = t \cdot P(\xi \geqslant t).
 \end{align*}
\end{proof}

Неравенство Маркова можно обобщить.

\begin{prop}
 Пусть $ \xi \geqslant 0 $ почти всюду, функция $f$ неотрицательна, возрастает, и число $ t > 0 $ таково, что $ f(t) > 0 $. Тогда
 \begin{align*}
  P(\xi \geqslant t) \leqslant \frac{\E f(\xi)}{f(t)}.
 \end{align*}
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Действительно,
 \begin{align*}
  P(\xi \geqslant t) = P(f(\xi) \geqslant f(t)) \leqslant \frac{\E f(\xi)}{f(t)}
 \end{align*} по обычному неравенству \eqref{equation:markov_inequality} Маркова.
\end{proof}

Математическое ожидание характеризует среднее значение случайной величины --- константу, которая в некотором смысле лучше всего описывает распределение случайной величины. 

Посмотрим на другие характеристики случайной величины.

\begin{df}[моменты случайной величины]
 Пусть есть целое число $ k \geqslant 1 $.
 \begin{itemize}
  \item \textit{$ k $-м моментом} случайной величины $ \xi $ называется число
   \begin{align*}
    \E \xi^{k} = \int\limits_{\R} x^{k}\,dP_\xi(x). 
   \end{align*}
  \item \textit{$ k $-м абсолютным моментом} называется число
   \begin{align*}
    \E \left| \xi \right|^{k} = \int\limits_{\R} \left| x \right|^{k}dP_\xi(x). 
   \end{align*}
  \item \textit{$ k $-м центральным моментом} называется число
   \begin{align*}
    \E (\xi - \E \xi)^{k}.
   \end{align*}
  \item \textit{$ k $-м абсолютным центральным моментом} называется число
   \begin{align*}
    \E \left| \xi - \E\xi \right|^{k}.
   \end{align*}
 \end{itemize}
\end{df}

Отметим, что в случае, когда $ k $  чётное, абсолютный момент равен обычному.

Другая известная характеристика случайной величины --- медиана.

\begin{df}[медиана случайной величины]
 Число $ m \in\R$ называется \textit{медианой} cлучайной величины $ \xi $, если
 \begin{align*}
  P(\xi \leqslant m) \geqslant \frac{1}{2}, & &\text{ и } & &P(\xi \geqslant m) \geqslant \frac{1}{2}.
 \end{align*} 
\end{df}
\begin{remrk*}
 Знак неравенства в определении медианы нужен для дискретных распределений, в случае когда $ P(\xi = m) > 0 $. Например, у константной случайной величины $ \xi = 0 $ число $ m = 0 $ не было бы медианой, если бы там стояли знаки равенства (ведь $ P(\xi \geqslant 0) = P(\xi \leqslant 0) = 1 \neq 1 / 2$).
\end{remrk*}
\begin{remrk*}
 Медиана не единственна. Возьмём бросок честной монетки: любое число из отрезка $ [0,1] $ будет медианой (слева от него число $ 0 $ с вероятностью $ 1 / 2 $, а справа от него число $ 1 $ с вероятностью $ 1 / 2 $). 
\end{remrk*}
\begin{remrk*}
 Легко видеть, что множество всех медиан образует отрезок $ [m_1,m_2] $ (возможно вырождающийся в одно число, когда $ m_1 = m_2 $). Иногда фиксируют одну медиану --- середину этого отрезка.
\end{remrk*}

\begin{remrk*}
 Если $ F_\xi $ --- непрерывная функция распределения случайной величины $ \xi $, то отрезок $ [m_1,m_2] = F_\xi^{-1}(1/2) $ содержит всевозможные медианы величины $ \xi $.
\end{remrk*}

Основная проблема медианы состоит в том, что её сложно вычислять. Однако, в некоторых случаях медиана может описывать статистические данные лучше математического ожидания.

\begin{exmpl*}
 В компании есть $ 1000 $ человек, один из них --- начальник, а остальные $ 999 $ --- подчинённые. У подчинённых зарплата $ 1000 $ долларов, а у начальника --- $ 1000000 $ долларов. Тогда математическое ожидание зарплаты равно $ 1999 $ долларов, а медиана --- $ 1000 $ долларов. Как можно видеть, в данном случае медиана лучше приближает <<среднюю зарплату>>.
\end{exmpl*}

\begin{remrk*}
 Медиану можно оценить сверху в терминах математического ожидания. Пусть $ \xi \geqslant 0 $ и $ \E\xi > 0 $. Тогда по неравенству Маркова
 \begin{align*}
  P(\xi \geqslant 2\E\xi) \leqslant \frac{\E\xi}{2\E\xi} = \frac{1}{2}.
 \end{align*} Следовательно, $ m \leqslant 2\E\xi $, где в левой части подразумевается наименьшая медиана.
\end{remrk*}

Существует обобщение понятия медианы.

\begin{df*}
 Пусть $ \alpha \in (0,1) $. Тогда \textit{$ \alpha $-квантилем} случайной величины $ \xi $ называется число $ x \in \R $ такое, что
 \begin{align*}
  P(\xi \leqslant x) \leqslant \alpha, &&\text{и} &&P(\xi \geqslant x) \geqslant 1 - \alpha.
 \end{align*}
\end{df*}

Медиана --- это в точности $ 1 / 2 $-квантиль.

До текущего момента мы рассматривали характеристики случайной величины, приближающие её <<среднее значение>>. Сейчас мы рассмотрим \textit{дисперсию} --- каноническую характеристику, описывающую степень отклонения случайной величины от её математического ожидания.

\begin{df}[дисперсия]
 \textit{Дисперсией} случайной величины $ \xi $ называется её второй центральный момент:
 \begin{align*}
  \mathbb D \xi = \E (\xi - \E\xi)^{2}.
 \end{align*}
 В англоязычной литературе дисперсию обозначают $ \mathrm{Var} \;\xi $.
\end{df}

Рассмотрим некоторые свойства дисперсии.
\begin{prop}[свойства дисперсии]\
 \begin{enumerate}
  \item $ \D\xi = \E\xi^{2} - (\E\xi)^{2} $.
   \begin{proof}[\normalfont\textsc{Доказательство}]
    Обозначим $ a = \E\xi $. Тогда
    \begin{align*}
     \D\xi &= \E(\xi - a)^{2} = \E(\xi^{2} - 2a\xi + a^{2}) = \E\xi^{2} - 2a\E\xi + a^{2} = \E\xi^{2} - a^{2}.
    \end{align*}
   \end{proof}
  \item $ \D\xi \geqslant 0 $.
  \item Если $ \D\xi = 0 $, то $ P(\xi = \E\xi) = 1 $ (случайная величина почти константа).
   \begin{proof}[\normalfont\textsc{Доказательство}]
    Так как интеграл от неотрицательной функции равен нулю:
    \begin{align*}
     \int\limits_{\Omega} (\xi-\E\xi)^{2} \,dP = 0,
    \end{align*} то подынтегральная функция равна нулю почти всюду: $ (\xi - \E\xi)^{2} = 0 $ почти всюду.
   \end{proof}
  \item \label{enum:variance_plus_const} $ \D(\xi + c) = \D\xi $.
   \begin{proof}[\normalfont\textsc{Доказательство}]
    \begin{align*}
     \D(\xi + c) = \E(\xi + c - \E(\xi + c))^{2} = \E(\xi - \E\xi)^{2} = \D\xi.
    \end{align*}
   \end{proof}
  \item \label{enum:variance_times_const} $ \D(c\xi) = c^{2}\D\xi $. В частности, $ \D(-\xi) = \D\xi $.
   \begin{proof}[\normalfont\textsc{Доказательство}]
    \begin{align*}
     \D(c\xi) = \E(c\xi - \E(c\xi))^{2} = \E (c^{2}(\xi - \E\xi)^{2}) = c^{2}\E(\xi-\E\xi)^{2} = c^{2}\D\xi.
    \end{align*}
   \end{proof}
 \end{enumerate}
\end{prop}

Как видно из свойств \ref{enum:variance_plus_const}, \ref{enum:variance_times_const}, дисперсия не линейна. Однако для независимых величин верно следующее.

\begin{prop}
 Если cлучайные величины $ \xi  $ и $ \eta $ независимы, то
 \begin{align*}
  \D(\xi + \eta) = \D\xi + \D\eta.
 \end{align*}
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 \begin{align*}
  \D(\xi + \eta) &= \E(\xi + \eta)^{2} - \left( \E(\xi+\eta) \right)^{2} = \\
  &= \E\xi^{2} + 2\E(\xi\eta) + \E\eta^{2} -  (\E\xi)^{2} - 2\E\xi \cdot \E\eta - (\E\eta)^{2} = \\
  &=  \E\xi^{2} + \E\eta^{2} -  (\E\xi)^{2} - (\E\eta)^{2} = \\
  &=\D\xi+\D\eta.
 \end{align*} $ \E(\xi\eta) = \E\xi \cdot \E\eta $ по независимости.
\end{proof}

\begin{prop}
 $ \E \left| \xi - \E\xi \right| \leqslant \sqrt{\D\xi} $.
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Это неравенство Ляпунова \eqref{proposition:Lyapunov_inequality} для $ 1 $ и $ 2 $.
\end{proof}

\begin{prop}[неравенство Чебышева]
 \label{proposition:chebyshev_inequality}
 $ t > 0 $
 \begin{align}
  \label{equation:chebyshev_inequality}
  P(\left| \xi - \E\xi \right| \geqslant t) \leqslant \frac{\D\xi}{t^{2}}
 \end{align} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Подставим в неравенство \eqref{equation:markov_inequality} Маркова $ p=2 $ и величину $ \eta = \left| \xi - \E\xi \right| $. Тогда
 \begin{align*}
  P(\eta \geqslant t) \leqslant \frac{\E\eta^{2}}{t^{2}} = \frac{\D\xi}{t^{2}}.
 \end{align*} 
\end{proof}

Рассмотрим примеры для конкретных распределений.

\begin{exmpl}
 $ \xi \sim U[0,1] $. Тогда
 \begin{align*}
  \E\xi &= \int\limits_{\R} x\,dP_\xi(x)  = \int\limits_{\R} x \cdot \Ind_{[0,1]}  (x)\,dx= \int\limits_{0}^{1} x\,dx = \left. \frac{x^{2}}{2} \right|_0^{1} = \frac{1}{2}; \\
   \E\xi^{2} &= \int\limits_{\R} x^2\,dP_\xi(x) = \int\limits_{0}^{1} x^{2}\,dx = \left. \frac{x^{3}}{3}\right|_0^{1} = \frac{1}{3}; \\
    \D\xi &=\E\xi^{2} - \left( \E\xi \right)^{2} = \frac{1}{3} - \left(\frac{1}{2}\right)^2 = \frac{1}{12}.
   \end{align*} 
  \end{exmpl}
  \begin{exmpl}
   $ \xi \sim U[a,b] $. Если $ \eta \sim U[0,1] $, то $ \xi = (b-a)\eta + a\sim U[a,b] $. Тогда
   \begin{align*}
    \E\xi &= \E \left( (b-a)\eta + a \right) = (b-a)\E\eta + a = \frac{b-a}{2} + a = \frac{a+b}{2}; \\
    \D\xi &= \D((b-a)\eta + a) = (b-a)^{2}\D\eta = \frac{(b-a)^{2}}{12}.
   \end{align*} 
  \end{exmpl}

  \begin{exmpl}
   Пусть $ \xi \sim \Norm(0,1) $,  $ p_\xi = \frac{1}{\sqrt{2\pi}}e^{-t^{2} / 2} $. Тогда
   \begin{align*}
    \E\xi &= \int\limits_{\R} x\,dP_\xi(x) = \int\limits_{\R} x p_\xi(x)\,dx = \int\limits_{\R} x \cdot \frac{1}{\sqrt{2\pi}}      e^{-x^{2} / 2}\,dx = 0,
   \end{align*} так как функция нечётная. Тогда
   \begin{align*}
    \D\xi &= \E\xi^{2} = \frac{1}{\sqrt{2\pi}}\int\limits_{\R} x^{2} e^{-x^{2} / 2}\,dx = [xe^{-\frac{x}{2}} = (-e^{-x^{2} / 2})'] = \\
    &= \frac{1}{\sqrt{2\pi}} \left( x \left.(-e^{-x^{2} / 2})\right|_{-\infty}^{+\infty} + \int\limits_{\R} e^{-x^{2} / 2}\,dx  \right) = \\
     &= \frac{1}{\sqrt{2\pi}} \int\limits_{\R} e^{-x^{2} / 2} \,dx = 1,
    \end{align*} так как это плотность распределения.
   \end{exmpl}

   \begin{exmpl}
    $ \xi \sim \Norm(\mu,\sigma^{2}) $. Тогда
    \begin{align*}
     p_\xi(t) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^{2}}.
    \end{align*} Если $ \eta \sim \Norm(0,1) $, то $ \xi = \sigma \eta + a \sim \Norm(\mu,\sigma^{2}) $.
    \begin{align*}
     &F_\xi(x) = P(\xi \leqslant x) = P(\sigma\eta + \mu \leqslant x) = P\left(\eta \leqslant \frac{x-\mu}{\sigma}\right) = F_{\eta}\left( \frac{x-\mu}{\sigma} \right) = \\
     &= \frac{1}{\sqrt{2\pi}} \int\limits_{-\infty}^{\frac{x-\mu}{\sigma}} e^{-t^{2} / 2}\,dt. \\
     &p_\xi(x) = F'_\xi(x) = \frac{1}{\sqrt{2\pi}} \cdot e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^{2}} \left( \frac{x-\mu}{\sigma} \right)' = \frac{1}{\sigma\sqrt{2\pi}} \cdot e^{-\frac{1}{2} \left( \frac{x-\mu}{\sigma} \right)^{2}}.
    \end{align*} Тогда
    \begin{align*}
     &E\xi = \E(\sigma \eta + \mu)  = \sigma \E\eta + \mu = \mu.\\
     &D\xi = \D(\sigma\eta + a) = \sigma^{2}\D\eta = \sigma^{2}.
    \end{align*} 
   \end{exmpl}

   \begin{df}[ковариация]
    Пусть $ \xi $ и $ \eta $ --- случайные величины, $ \E\xi^{2} $, $ \E\eta^{2} < +\infty$. Тогда \textit{ковариацией} $ \xi $ и $ \eta $ называется
    \begin{align*}
     \mathrm{cov}(\xi,\eta) = \E \left( (\xi - \E\xi)(\eta - \E\eta) \right)
    \end{align*} 
   \end{df}
   \begin{prop}
    \begin{align*}
     \cov(\xi,\eta) = \E(\xi\eta) - \E\xi \cdot \E\eta.
    \end{align*} 
   \end{prop}
   \begin{proof}[\normalfont\textsc{Доказательство}]
    Обозначим $\E \xi = a, \E \eta = b$ 
    \begin{align*}
     \cov(\xi, \eta) = \E((\xi - a)(\eta - b)) = \E(\xi \eta) - a\underbrace{\E\eta}_b -b\underbrace{\E\xi}_a + a \cdot b = \E(\xi \eta) - a \cdot b
    \end{align*}
   \end{proof}
   \begin{prop}
    \begin{align*}
     \cov(\xi,\xi) = \D\xi.
    \end{align*} 
   \end{prop}
   \begin{prop}
    \begin{align*}
     \cov(\xi,\eta) = \cov(\eta,\xi).
    \end{align*}
   \end{prop}
   \begin{prop}
    \begin{align*}
     \cov(a\xi_1 + b\xi_2, \eta) = a\cov(\xi_1,\eta) + b \cov(\xi_2, \eta).
    \end{align*} 
   \end{prop}
   \begin{prop}
    Если $ \xi $ и $ \eta $ независимы, то
    \begin{align*}
     \cov(\xi,\eta) = 0.
    \end{align*} 
   \end{prop}
   \begin{prop}
    $ \D(\xi + \eta) = \D\xi + \D\eta + 2\cov(\xi,\eta) $.
   \end{prop}
   \begin{proof}[\normalfont\textsc{Доказательство}]
    \begin{align*}
     \D(\xi+\eta) &= \E(\xi+\eta)^{2} - \left( \E(\xi+\eta) \right)^{2} = \\
     &= \E\xi^{2} + \E\eta^{2} + 2\E(\xi\eta) - (\E\xi)^{2} - (\E\eta)^{2} - 2\E\xi \cdot \E\eta = \\
     &= \D\xi + \D\eta +2\cov(\xi,\eta).
    \end{align*} 
   \end{proof}
   \begin{prop}
    \begin{align*}
     \D(\xi_1 + \xi_2 + \ldots + \xi_n) = \sum_{i=1}^{n} \D\xi_i + 2 \sum_{i < j} \cov(\xi_i, \xi_j).
    \end{align*} 
   \end{prop}

   \begin{remrk}
    Из того, что $ \cov(\xi,\eta) = 0 $ не следует, что $ \xi $ и $ \eta $ независимы!
   \end{remrk}
   \begin{exmpl}
    Пусть $ \Omega = \left\{ 0,\frac{\pi}{2},\pi \right\} $ с одинаковыми вероятностями. Рассмотрим две случайной величины
    \begin{align*}
     \xi(\omega) = \cos \omega, & &\eta(\omega) = \sin \omega.
    \end{align*} Тогда
    \begin{align*}
     \E\xi = \frac{1 + 0 + (-1)}{3} = 0,
    \end{align*} следовательно $ \E\xi \cdot \E\eta = 0 $. Далее,
    \begin{align*}
     \xi\eta = 0, 
    \end{align*} значит $ \E(\xi\eta) = 0 $. Следовательно, $ \cov(\xi,\eta) = 0 $, но нет независимости!

    \begin{align*}
     P(\xi = 1,\eta =1) = 0 \neq \frac{1}{3} \cdot \frac{1}{3} = P(\xi = 1) \cdot P(\eta = 1).
    \end{align*}
   \end{exmpl}

   \begin{df}[коэффициент корреляции]

    \begin{flalign*}
     &  \rho(\xi,\eta) = \frac{\cov(\xi,\eta)}{\sqrt{\D\xi} \cdot \sqrt{\D\eta}} \in [-1,1] &\\
     &  |\cov(\xi, \eta)| \leq \E (|\xi - \E \xi| \cdot |\eta - \E \eta|) \leq (\E |\xi - \E \xi|^2)^{\frac{1}{2}} \cdot (\E |\eta - \E \eta|^2)^{\frac{1}{2}} = \sqrt{\D\xi}\cdot\sqrt{\D\eta}. &
    \end{flalign*} Если $ \rho(\xi,\eta) = 0 $, то величины $ \xi $ и $ \eta $ называются \textit{некоррелированными}.

   \end{df}

   \begin{exercs*}
    Пусть $ \rho(\xi,\eta) = \pm 1 $. Доказать, что тогда $ \xi = a\eta + b $, где $ a, b \in \R $ и $ \mathrm{sign}\;a = \mathrm{sign}\;\rho(\xi,\eta) $.
   \end{exercs*}

   \end{document}
