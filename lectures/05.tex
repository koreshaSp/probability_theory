% 2023.03.14: lecture 05
\documentclass[../main.tex]{subfiles}
\begin{document}

\newpage
\section{Математическое ожидание и дисперсия.}

В этом параграфе мы определим ключевые характеристики случайных величин: математическое ожидание, дисперсию, моменты и другие.

\subsection{Математическое ожидание.}

\begin{df}[математическое ожидание]
 \textit{Математическим ожиданием} (на жаргоне \textit{матожиданием}) cлучайной величины $ \xi \colon \, \Omega \to \R $ называется число
 \begin{align}
  \label{equation:expected_value_def}
  \E \xi = \int_{\Omega}  \xi\,dP,
 \end{align} где интеграл берётся по мере $ P $ --- вероятности.
\end{df}
\begin{remrk*}
 В общем случае у случайной величины может не быть математического ожидания! Такое может быть, когда интеграл \eqref{equation:expected_value_def} не существует (расходится). Например, распределение Коши не имеет матожидания.

 Если $ \xi \geqslant 0 $ с вероятностью $ 1 $, то можно считать, что матожидание $ \E \xi $ существует всегда, но тогда возможен случай $ \E \xi = \infty $ (интеграл расходится).
\end{remrk*}

К математическому ожиданию стоит относится как к среднему значению случайной величины: одному вещественному числу, которое <<наилучшим образом>> приближает случайную величину.

В следующем предложении перечислим свойства матожидания, непосредственно следующие из свойств интеграла Лебега.
\begin{prop}\
 \begin{enumerate}
  \item Математическое ожидание линейно:
   \begin{align*}
    \E(a\xi + b\eta) = a \E \xi + b \E \eta
   \end{align*} для любых случайных величин $ \xi,\eta $ и чисел $ a,b\in \R $. Более формально, левая часть определена тогда и только тогда, когда определены оба слагаемых в правой части, и в этом случае выполнено равенство.
  \item Если $ \xi \geqslant 0 $ с вероятностью $ 1 $, то $ \E\xi\geqslant 0 $.
  \item Если $ \xi \leqslant \eta $ с вероятностью $ 1 $, то $ \E\xi \leqslant \E\eta $.
 \end{enumerate}
\end{prop}

Важное замечание: линейность матожидания верна даже если величины $ \xi $ и $ \eta $ не являются независимыми! Это крайне важное и часто полезное свойство..

Математическое ожидание можно выразить, вообще не привлекая исходное вероятностное пространство, исключительно в терминах распределения.
\begin{prop}
 Если математическое ожидание cлучайной величины $ \xi $ существует, то верна формула
 \begin{align}
  \label{equation:expected_value_integral_on_real}
  \E\xi = \int_{\R} x\,dP_{\xi}(x),
 \end{align} где $ P_\xi $ --- распределение случайной величины $ \xi $ как борелевская мера на $ \R $.
\end{prop}
Формула \eqref{equation:expected_value_integral_on_real} будет частным случаем следующей крайне полезной формулы.
\begin{prop}
 Если $ \xi_1, \xi_2, \ldots, \xi_n \colon\, \Omega \to \R $ --- случайные величины, а $ f \colon\, \R^{n}\to\R $ --- измеримая по Борелю функция, то
 \begin{align}
  \label{equation:expected_value_of_function}
  \E f(\xi_1, \xi_2, \ldots, \xi_n) = \int_{\R^{n}} f(x_1, \ldots, x_n) \,d P_{\vec\xi}(x_1, \ldots, x_n),
 \end{align} где $ P_{\vec\xi} $ --- совместное распределение случайных величин $ \vec\xi = (\xi_1, \xi_2, \ldots, \xi_n) $ как борелевская мера на $ \R^{n} $.
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Доказательство будет по стандартной схеме из теории меры.
 \begin{itemize}
  \item Пусть $ f = \Ind_A $ --- индикаторная функция, где $ A \subset \R^{n} $ --- борелевское множество. Тогда
   \begin{align*}
    \E f (\xi_1, \ldots, \xi_n) = P(\vec\xi \in A) = P_{\vec\xi}(A) = \int_{\R^{n}} \Ind_A\,dP_{\vec\xi}.
   \end{align*}  Для индикаторных функций формула \eqref{equation:expected_value_of_function} доказана.
  \item По линейности матожидания и интеграла Лебега формула \eqref{equation:expected_value_of_function} доказана для простых неотрицательных функций.
  \item Пусть неотрицательная функция $ f \geqslant 0 $ измерима. По теореме об аппроксимации существуют простые неотрицательные функции $ f_1,f_2, \ldots $, поточечно возрастающие к $ f $. По предыдущему пункту для каждого $ k \geqslant 1 $ имеем \begin{align*}
    \E f_k(\xi_1, \xi_2, \ldots, \xi_n) = \int_{\R^{n}} f_k(\vec x) \,dP_{\vec\xi}(\vec x).
   \end{align*} По теореме Леви правая часть стремится к
   \begin{align*}
    \lim_{k \to \infty} \int_{\R^{n}} f_k(\vec x)  \,dP_{\vec\xi}(\vec x) = \int_{\R^{n}} f(\vec x)\,dP_{\vec\xi} (\vec x),
   \end{align*} а левая часть к
   \begin{align*}
    \lim_{k \to \infty} \E f_k (\xi_1, \ldots, \xi_n) &= \lim_{k \to \infty} \int_{\Omega} f_k(\xi_1, \ldots, \xi_n) \,dP = \int_{\Omega} f(\xi_1, \ldots, \xi_n)\,dP = \\
    &= \E f(\xi_1, \ldots, \xi_n).
   \end{align*} Равенство \eqref{equation:expected_value_of_function} доказано для неотрицательных измеримых функций.
  \item Для функций произвольного знака рассмотреть $ f_\pm = \max(\pm f, 0) $ и воспользоваться линейностью.
 \end{itemize}
\end{proof}
\begin{crly}
 Математическое ожидание зависит только от распределения случайной величины.
\end{crly}

Следующее свойство матожидания не имеет никакого аналога в теории меры, оно имеет смысл лишь в теории вероятностей.

\begin{prop}
 \label{proposition:independent_expected_product}
 Если случайные величины $ \xi $ и $ \eta $ независимы, то
 \begin{align}
  \label{equation:independednt_expected_product}
  \E(\xi \eta) = \E\xi \cdot \E\eta.
 \end{align} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Воспользуемся формулой \eqref{equation:expected_value_of_function} для функции $ f(x,y) = xy $; тем фактом, что совместное распределение $ \xi $ и $ \eta $ является произведением распределений из-за независимости (теорема \ref{theorem:random_variable_independence_through_distribution_measure_eq}), а также теоремой Фубини:
 \begin{align*}
  \E(\xi\eta) &= \int_{\R^{2}} xy \,dP_{(\xi,\eta)} (x,y) = \int_{\R^{2}} xy\,d(P_{\xi} \times P_{\eta})(x,y) = \int_{\R} \left( \int_{\R} y\,dP_\eta(y)  \right)x\, dP_\xi(x) = \\
  &=  \int_{\R} x\,dP_\xi(x)  \cdot  \int_{\R} y\,dP_\eta(y)   = \E \xi \cdot \E\eta.
 \end{align*} 
\end{proof}
\begin{crly}
 Если cлучайные величины $ \xi_1,\xi_2, \ldots, \xi_n $ независимы, то
 \begin{align*}
  \E(\xi_1 \xi_2 \ldots \xi_n) = \E\xi_1 \cdot \E\xi_2 \cdot \ldots \cdot \E\xi_n.
 \end{align*} 
\end{crly}
\begin{remrk}
 Формула \eqref{equation:independednt_expected_product} не верна без независимости случайных величин $ \xi $ и $ \eta $.
\end{remrk}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Пусть $ \xi, \eta $ равны $ \pm 1 $ с вероятностью $ \frac{1}{2} $. Тогда $ \E\xi = \E\eta = 0 $. Но если $ \xi = \eta $, то $ \E(\xi\eta) = \E(1) = 1 $!
\end{proof}
\begin{remrk}
 Из равенства \eqref{equation:independednt_expected_product} не следует независимость случайных величин $ \xi $ и $ \eta $.
\end{remrk}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Пусть пространство элементарных исходов состоит из четырёх равновероятных исходов: $ \Omega = \left\{ \omega_1, \omega_2, \omega_3, \omega_4 \right\} $, а случайные величины $ \xi $ и $ \eta $ принимают следующие значения:
 \begin{center}
  \begin{tabular}{r|cccc}
   & $\omega_1$ & $\omega_2$ & $\omega_3$ & $\omega_4$ \\
   \hline
   $\xi$ & $ 1 $ & $0$ & $0$ & $-1$ \\
   $\eta$ & $0$ & $1$ & $-1$ & $0$
  \end{tabular}
 \end{center}
 Тогда  $ \E \xi = \frac{1 + 0 + 0 + (-1)}{4} = 0 $, $ \E\eta = \frac{0 + 1 + (-1) + 0}{4} = 0 $, $ \E (\xi \eta) = \frac{0 + 0 + 0 + 0}{4} = 0 $, и равенство \eqref{equation:independednt_expected_product} выполнено. Однако независимости нет: например события $ \left\{ \xi=1 \right\} $ и $ \left\{ \eta = 0 \right\} $ зависимы.
\end{proof}

\begin{prop}
 Если $ \xi \geqslant 0 $ почти всюду, то
 \begin{align*}
  \E\xi = \int_{0}^{\infty} P(\xi \geqslant t)\,dt.
 \end{align*} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Применим хитрым образом теорему Тонелли:
 \begin{align*}
  \E\xi &= \int_{\Omega} \xi(\omega)\,dP(\omega) = \int_{\Omega} \int_{0}^{\xi(\omega)} dt\,dP(\omega) = \int_{0}^{\infty} \int_{\Omega}   \Ind_{[0, \xi(\omega)]}(t)\,dP(\omega)\,dt = \\
  &= \int_{0}^{\infty} P(\xi(\omega) \geqslant t)\,dt.
 \end{align*} 
\end{proof}

\subsection{Неравенства с математическим ожиданием.}

\begin{prop}[неравенство Гёльдера]
 \label{proposition:gulder_inequality}
 Если числа $ p, q > 1 $ таковы, что $ 1/p+1/q = 1 $, то
 \begin{align}
  \label{equation:holder_inequality}
  \E\left|\xi\eta\right| \leqslant \left( \E \left| \xi \right|^{p} \right)^{1/p} \left( \E \left| \eta \right|^{q} \right)^{1/q}.
 \end{align} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Применим неравенство Гёльдера для интегралов:
 \begin{align*}
  \int_{\Omega} \left|\xi \eta \right|\,dP \leqslant \left( \int_{\Omega} \left| \xi \right|^{p}  \right)^{1 / p} \left( \int_{\Omega} \left| \eta \right|^{q}  \right)^{1 / q}.
 \end{align*}
\end{proof}

\begin{prop}[неравенство Ляпунова]
 \label{proposition:Lyapunov_inequality}
 Если $ 0 < r < s $, то 
 \begin{align}
  \label{eq:lyapunov_inequality}
  \left(\E \left| \xi \right|^{r} \right)^{1/r} \leqslant \left( \E \left| \xi \right|^{s} \right)^{1/s}.
 \end{align} 
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Возведём обе части в степень $r$. Нужно проверить 
 \begin{align*}
  \E \left|\xi\right|^r \leqslant (\E \left|\xi\right|^s)^{r/s}
 \end{align*}

 Воспользуемся неравенством Гёльдера \eqref{equation:holder_inequality}, взяв $ \xi = \left| \xi \right|^{r} $, $ \eta = 1 $, $ p = s / r > 1 $ и соответствующее $ q $:
 \begin{align*}
  \E \left| \xi \right|^{r} \leqslant \left[ \E (\left| \xi \right|^{r})^{s / r} \right]^{r / s} \cdot \left( \E 1^{q} \right)^{1 / q} = \left( \E \left| \xi \right|^{s} \right)^{r / s}.
 \end{align*}
\end{proof}

\begin{prop}[неравенство Маркова]
 Пусть $ \xi \geqslant 0 $ почти всюду и число $ t > 0 $. Тогда
 \begin{align}
  \label{equation:markov_inequality}
  P(\xi \geqslant t) \leqslant \frac{\E \xi}{t}.
 \end{align}  
\end{prop}

Неравенство \eqref{equation:markov_inequality} Маркова было в теории меры (там оно называлось неравенством Чебышева; в теории вероятностей такое название будет у другого неравенства).

\begin{proof}
 \begin{align*}
  \E \xi = \int_{\Omega} \xi\,dP \geqslant \int_{\left\{ \xi \geqslant t \right\}} \xi \,dP \geqslant \int_{\left\{ \xi \geqslant t \right\}}  t\,dP = t \cdot P(\xi \geqslant t).
 \end{align*}
\end{proof}

Неравенство Маркова можно обобщить.

\begin{prop}
 Пусть $ \xi \geqslant 0 $ почти всюду, функция $f \colon\, [0, +\infty) \to [0, +\infty)$ неотрицательна, строго возрастает, и число $ t > 0 $. Тогда
 \begin{align}
  \label{eq:general_markov_ineq}
  P(\xi \geqslant t) \leqslant \frac{\E f(\xi)}{f(t)}.
 \end{align}
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Действительно,
 \begin{align*}
  P(\xi \geqslant t) = P(f(\xi) \geqslant f(t)) \leqslant \frac{\E f(\xi)}{f(t)}
 \end{align*} по обычному неравенству \eqref{equation:markov_inequality} Маркова.
\end{proof}

\subsection{Медиана.}

Другая известная характеристика случайной величины --- медиана.

\begin{df}[медиана случайной величины]
 Число $ m \in\R$ называется \textit{медианой} cлучайной величины $ \xi $, если
 \begin{align*}
  P(\xi \leqslant m) \geqslant \frac{1}{2}, & &\text{ и } & &P(\xi \geqslant m) \geqslant \frac{1}{2}.
 \end{align*} 
\end{df}
\begin{remrk*}
 Знак неравенства в определении медианы нужен для дискретных распределений, в случае когда $ P(\xi = m) > 0 $. Например, у константной случайной величины $ \xi = 0 $ число $ m = 0 $ не было бы медианой, если бы там стояли знаки равенства (ведь $ P(\xi \geqslant 0) = P(\xi \leqslant 0) = 1 \neq 1 / 2$).
\end{remrk*}
\begin{remrk*}
 Медиана не единственна. Возьмём бросок честной монетки: любое число из отрезка $ [0,1] $ будет медианой (слева от него число $ 0 $ с вероятностью $ 1 / 2 $, а справа от него число $ 1 $ с вероятностью $ 1 / 2 $). 
\end{remrk*}
\begin{remrk*}
 Легко видеть, что множество всех медиан образует отрезок $ [m_1,m_2] $ (возможно вырождающийся в одно число, когда $ m_1 = m_2 $). Иногда фиксируют одну медиану --- середину этого отрезка.
\end{remrk*}

\begin{remrk*}
 Если $ F_\xi $ --- непрерывная функция распределения случайной величины $ \xi $, то отрезок $ [m_1,m_2] = F_\xi^{-1}(1/2) $ содержит всевозможные медианы величины $ \xi $.
\end{remrk*}

Основная проблема медианы состоит в том, что её сложно вычислять. Однако, в некоторых случаях медиана может описывать статистические данные лучше математического ожидания.

\begin{exmpl*}
 В компании есть $ 1000 $ человек, один из них --- начальник, а остальные $ 999 $ --- подчинённые. У подчинённых зарплата $ 1000 $ долларов, а у начальника --- $ 1000000 $ долларов. Тогда математическое ожидание зарплаты равно $ 1999 $ долларов, а медиана --- $ 1000 $ долларов. Как можно видеть, в данном случае медиана лучше приближает <<среднюю зарплату>>.
\end{exmpl*}

\begin{remrk*}
 Медиану можно оценить сверху в терминах математического ожидания. Пусть $ \xi \geqslant 0 $ и $ \E\xi > 0 $. Тогда по неравенству Маркова
 \begin{align*}
  P(\xi \geqslant 2\E\xi) \leqslant \frac{\E\xi}{2\E\xi} = \frac{1}{2}.
 \end{align*} Следовательно, $ m \leqslant 2\E\xi $, где в левой части подразумевается наименьшая медиана.
\end{remrk*}

Существует обобщение понятия медианы.

\begin{df*}
 Пусть $ \alpha \in (0,1) $. Тогда \textit{$ \alpha $-квантилем} случайной величины $ \xi $ называется число $ x \in \R $ такое, что
 \begin{align*}
  P(\xi \leqslant x) \leqslant \alpha, &&\text{и} &&P(\xi \geqslant x) \geqslant 1 - \alpha.
 \end{align*}
\end{df*}

Медиана --- это в точности $ 1 / 2 $-квантиль.

\subsection{Моменты случайной величины.}

\begin{df}[моменты случайной величины]
 Пусть случайная величина $ \xi $ имеет матожидание $ \E\xi $, и пусть дано целое число $ k \geqslant 1 $.
 \begin{itemize}
  \item \textit{$ k $-м моментом} (\textit{$ k $-м начальным моментом}) случайной величины $ \xi $ называется число
   \begin{align*}
    \E \xi^{k} = \int_{\R} x^{k}\,dP_\xi(x). 
   \end{align*}
  \item \textit{$ k $-м абсолютным моментом} называется число
   \begin{align*}
    \E \left| \xi \right|^{k} = \int_{\R} \left| x \right|^{k}dP_\xi(x). 
   \end{align*}
  \item \textit{$ k $-м центральным моментом} называется число
   \begin{align*}
    \E (\xi - \E \xi)^{k}.
   \end{align*}
  \item \textit{$ k $-м абсолютным центральным моментом} называется число
   \begin{align*}
    \E \left| \xi - \E\xi \right|^{k}.
   \end{align*}
 \end{itemize}
\end{df}

\begin{remrk*}
 В случае, когда $ k $ чётное, $ k $-й абсолютный момент равен обычному $ k $-му моменту (а также $ k $-й абсолютный центральный момент равен $ k $-му центральному).
\end{remrk*}

\begin{remrk*}
 В общем случае моменты у случайной величины могут не существовать. Абсолютные моменты всегда либо существуют, либо равны $ +\infty $.
\end{remrk*}

\begin{remrk}
 Существование $ k $-го момента $\E\xi^{k}$ равносильно существованию $ k $-го абсолютного момента $ \E \left|\xi \right|^{k} $. Действительно, интеграл
 \begin{align*}
  \int_{\R} x^{k} \,dP_\xi(x)
 \end{align*} определён, если и только если функция $ x^{k} $ суммируема относительно меры $ P_\xi $, то есть сходится следующий интеграл:
 \begin{align*}
  \int_{\R} \left| x^{k} \right| dP_\xi(x) < \infty.
 \end{align*}
\end{remrk}

\begin{remrk}
 \label{remark:high_moment_exists_then_low_moment_exists}
 Предположим, что у величины $ \xi $ существует $ k $-й абсолютный момент $ \E \left| \xi \right|^{k} $. Тогда для всех $ l \leqslant k $ у $ \xi $ существует $ l $-й абсолютный момент $ \E \left| \xi \right|^{l} $.
\end{remrk}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Применить неравенство Ляпунова \eqref{eq:lyapunov_inequality}
\end{proof}

\begin{remrk}
 Существование $ k $-го момента $ \E\xi^{k} $ равносильно существованию $ k $-го центрального момента $ \E(\xi-\E\xi)^{k} $. Действительно, по линейности матожидания $ k $-й центральный момент можно выразить через начальные момента порядка не более $ k $:
 \begin{align*}
  \E(x-\E\xi)^{k} &= \E \left[ \sum_{j=0}^{k} x^{j}(-\E\xi)^{k-j} \right] = \sum_{j=0}^{k}(-\E\xi)^{k-j} \E x^{j}.
 \end{align*}
\end{remrk}

Таким образом, если для некоторого $ k $ у случайной величины $ \xi $ есть хотя бы один из четырёх видов моментов, то у неё есть все четыре вида моментов, причём для всех порядков $ l \leqslant k $.

\subsection{Дисперсия.}

До текущего момента мы рассматривали характеристики случайной величины, приближающие её <<среднее значение>>. Сейчас мы рассмотрим \textit{дисперсию} --- каноническую характеристику, описывающую степень отклонения случайной величины от её среднего.

\begin{df}[дисперсия]
 Пусть случайная величина $ \xi $ имеет матожидание $ \E\xi $. Тогда \textit{дисперсией} случайной величины $ \xi $ называется её второй центральный момент:
 \begin{align*}
  \mathbb D \xi = \E (\xi - \E\xi)^{2}.
 \end{align*}
 В англоязычной литературе дисперсию обозначают $ \mathrm{Var} \;\xi $.
\end{df}
\begin{prop}[свойства дисперсии]\
 \begin{enumerate}
  \item $ \D\xi = \E\xi^{2} - (\E\xi)^{2} $.
  \item $ \D\xi \geqslant 0 $.
  \item Если $ \D\xi = 0 $, то $ P(\xi = \E\xi) = 1 $ (случайная величина почти постоянна).
  \item \label{enum:variance_plus_const} $ \D(\xi + c) = \D\xi $.
  \item \label{enum:variance_times_const} $ \D(c\xi) = c^{2}\D\xi $. В частности, $ \D(-\xi) = \D\xi $.
 \end{enumerate}
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]\
 \begin{enumerate}
  \item Обозначим $ a = \E\xi $. Тогда
   \begin{align*}
    \D\xi &= \E(\xi - a)^{2} = \E(\xi^{2} - 2a\xi + a^{2}) = \E\xi^{2} - 2a\E\xi + a^{2} = \E\xi^{2} - a^{2}.
   \end{align*}
  \item Дисперсия --- интеграл от неотрицательной функции.
  \item Раз интеграл от неотрицательной функции равен нулю:
   \begin{align*}
    \int_{\Omega} (\xi-\E\xi)^{2} \,dP = 0,
   \end{align*} то подынтегральная функция равна нулю почти всюду: $ (\xi - \E\xi)^{2} = 0 $ с вероятностью $ 1 $.
  \item Действительно,
   \begin{align*}
    \D(\xi + c) = \E(\xi + c - \E(\xi + c))^{2} = \E(\xi - \E\xi)^{2} = \D\xi.
   \end{align*}
  \item Действительно,
   \begin{align*}
    \D(c\xi) = \E(c\xi - \E(c\xi))^{2} = \E (c^{2}(\xi - \E\xi)^{2}) = c^{2}\E(\xi-\E\xi)^{2} = c^{2}\D\xi.
   \end{align*}
 \end{enumerate}
\end{proof}

Как видно из свойств \ref{enum:variance_plus_const} и \ref{enum:variance_times_const}, дисперсия не обладает линейностью. Однако, для независимых случайных величин верно следующее.

\begin{prop}[дисперсия суммы независимых величин]
 Если cлучайные величины $ \xi  $ и $ \eta $ независимы, то
 \begin{align*}
  \D(\xi + \eta) = \D\xi + \D\eta.
 \end{align*}
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 \begin{align*}
  \D(\xi + \eta) &= \E(\xi + \eta)^{2} - \left( \E(\xi+\eta) \right)^{2} = \\
  &= \E\xi^{2} + 2\E(\xi\eta) + \E\eta^{2} -  (\E\xi)^{2} - 2\E\xi \cdot \E\eta - (\E\eta)^{2} = \\
  &=  \E\xi^{2} + \E\eta^{2} -  (\E\xi)^{2} - (\E\eta)^{2} = \\
  &=\D\xi+\D\eta,
 \end{align*} где $ \E(\xi\eta) = \E\xi \cdot \E\eta $ верно по независимости.
\end{proof}

\begin{prop}
 $ \E \left| \xi - \E\xi \right| \leqslant \sqrt{\D\xi} $.
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Это в точности неравенство Ляпунова \eqref{eq:lyapunov_inequality} для случайной величины $ \xi - \E\xi $ и чисел $ r=1 $, $ s=2 $:
 \begin{align*}
  \left(\E \left| \xi-\E\xi \right|^{1} \right)^{1 / 1} \leqslant \left( \E \left| \xi-\E\xi \right|^{2} \right)^{1 / 2}.
 \end{align*}
\end{proof}

\begin{prop}[неравенство Чебышева]
 \label{proposition:chebyshev_inequality}
 \begin{align}
  \label{equation:chebyshev_inequality}
  P(\left| \xi - \E\xi \right| \geqslant t) \leqslant \frac{\D\xi}{t^{2}}
 \end{align} при $ t > 0 $.
\end{prop}
\begin{proof}[\normalfont\textsc{Доказательство}]
 Применим обобщённое неравенство Маркова \eqref{eq:general_markov_ineq} для случайной величины $ \left| \xi-\E\xi \right| $ и неотрицательной возрастающей функции $ f(x) = x^{2} $:
 \begin{align*}
  P(\left| \xi-\E\xi \right| \geqslant t) \leqslant \frac{\E f(\left| \xi-\E\xi \right|)}{f(t)} = \frac{\E \left| \xi-\E\xi \right|^{2}}{t^{2}} = \frac{\D\xi}{t^{2}}.
 \end{align*}
\end{proof}

\subsection{Матожидание и дисперсия некоторых распределений.}

Рассмотрим моменты некоторых конкретных распределений.

\begin{exmpl}[моменты стандартного равномерного распределения]
 Пусть случайная величина $ \xi \sim U[0,1] $ --- имеет стандартное непрерывное равномерное распределение. Тогда
 \begin{align*}
  \E\xi &= \int_{\R} x\,dP_\xi(x)  = \int_{\R} x \cdot \Ind_{[0,1]}  (x)\,dx= \int_{0}^{1} x\,dx = \left. \frac{x^{2}}{2} \right|_0^{1} = \frac{1}{2}, \\ 
   \E\xi^{2} &= \int_{\R} x^2\,dP_\xi(x) = \int_{0}^{1} x^{2}\,dx = \left. \frac{x^{3}}{3} \right|_0^{1} = \frac{1}{3},\\
    \D\xi &=\E\xi^{2} - \left( \E\xi \right)^{2} = \frac{1}{3} - \left(\frac{1}{2}\right)^2 = \frac{1}{12}.
   \end{align*} 
  \end{exmpl}

  \begin{exmpl}[моменты равномерного распределения]
   Пусть случайная величина $ \xi \sim U[a,b] $ имеет непрерывное равномерное распределение. Тогда
   \begin{align*}
    \xi = (b - a)\eta + a,
   \end{align*} где $ \eta \sim U[0,1] $. Действительно, по арифметическим свойствам плотности 
   \begin{align*}
    p_\eta(t) &= p_{\frac{\xi - a}{b - a}}(t) = (b-a)p_{\xi - a}((b-a)t) = (b-a)p_\xi((b-a)t + a) = \\
    &= (b-a) \frac{1}{b-a} \Ind_{[a,b]}((b-a)t + a) = \Ind_{[0, b - a]}((b-a)t) = \Ind_{[0,1]}(t).
   \end{align*}
   Тогда
   \begin{align*}
    \E\xi &= \E \left( (b-a)\eta + a \right) = (b-a)\E\eta + a = \frac{b-a}{2} + a = \frac{a+b}{2}; \\
    \D\xi &= \D((b-a)\eta + a) = (b-a)^{2}\D\eta = \frac{(b-a)^{2}}{12}.
   \end{align*} Матожидание $ \E\xi = \frac{a+b}{2} $ соответствует интуиции: средним значением равномерного распределения $ U[a,b] $ должна быть середина отрезка $ [a,b] $.
  \end{exmpl}

  \begin{exmpl}[моменты стандартного нормального распределения]
   Пусть $ \xi \sim \Norm(0,1) $ --- имеет стандартное нормальное распределение,  $ p_\xi(t) = \frac{1}{\sqrt{2\pi}}e^{-t^{2} / 2} $ --- его плотность распределения. Тогда
   \begin{align*}
    \E\xi &= \int_{\R} x\,dP_\xi(x) = \int_{\R} x \cdot p_\xi(x)\,dx = \int_{\R} x \cdot \frac{1}{\sqrt{2\pi}}      e^{-x^{2} / 2}\,dx = 0,
   \end{align*} так как подынтегральная функция нечётная. Тогда
   \begin{align*}
    \D\xi &= \E\xi^{2} = \frac{1}{\sqrt{2\pi}}\int_{\R} x^{2} e^{-x^{2} / 2}\,dx = \begin{bmatrix}
     xe^{-x^{2} / 2} = \left( -e^{-x^{2} / 2} \right)'
    \end{bmatrix} = \\
    &= \frac{1}{\sqrt{2\pi}} \left( x \left.(-e^{-x^{2} / 2})\right|_{-\infty}^{+\infty} + \int_{\R} e^{-x^{2} / 2}\,dx  \right) = \\
     &= \frac{1}{\sqrt{2\pi}} \int_{\R} e^{-x^{2} / 2} \,dx = 1.
    \end{align*} Последнее равенство верно, так как интеграл $ \frac{1}{\sqrt{2\pi}} \int_{\R} e^{-x^{2} / 2}\,dx  $ --- это интеграл плотности стандартного нормального распределения по всей вещественной прямой, он обязан быть равным единице.
   \end{exmpl}

   \begin{exmpl}[моменты нормального распределения]
    Пусть $ \xi \sim \Norm(\mu,\sigma^{2}) $ --- имеет нормальное распределение. Его плотность равна
    \begin{align*}
     p_\xi(t) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} \left( \frac{t-\mu}{\sigma} \right)^{2}}.
     \end{align*} Тогда \begin{align*}
     \xi = \sigma \eta + \mu,
    \end{align*} где $ \eta \sim \Norm(0,1) $. Действительно,
    \begin{align*}
     p_\eta(t) &= p_{\frac{\xi - \mu}{\sigma}}(t) = \sigma p_{\xi - \mu}(\sigma t) = \sigma p_\xi(\sigma t + \mu) = \frac{\sigma}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2} \left( \frac{(\sigma t + \mu) - \mu}{\sigma} \right)^{2}} = \frac{1}{\sqrt{2\pi}}e^{-t^{2} / 2}.
    \end{align*} Но тогда
    \begin{align*}
     \E\xi &= \E(\sigma \eta + \mu)  = \sigma \E\eta + \mu = \mu.\\
     \D\xi &= \D(\sigma\eta + \mu) = \sigma^{2}\D\eta = \sigma^{2}.
    \end{align*} 

    Таким образом, матожидание и дисперсия нормального распределения $ \Norm(\mu, \sigma^{2}) $ равны $ \mu $ и $ \sigma^{2} $ соответственно, то есть первому и второму параметру распределения, что удобно!
   \end{exmpl}

   Следующих примеров не было на лекциях.

   \begin{exmpl}[моменты биномиального распределения]
    Пусть $ \xi\sim\Binom(n,p) $, $ q = 1-p $. Тогда
    \begin{align*}
     \E\xi &= \sum_{k=0}^{n} k \cdot \binom n k p^{k} q^{n-k} = \sum_{k=1}^{n}\frac{n!}{(k-1)!(n-k)!}p^{k}q^{n-k} = np \sum_{k=1}^{n} \binom {n-1} {k-1} p^{k-1}q^{n-k} = \\
     &= np \sum_{j=0}^{n-1} \binom{n-1}j p^{j}q^{(n-1)-j} = np,
    \end{align*} так как
    \begin{align*}
     1 = (p+q)^{n-1} = \sum_{j=0}^{n-1} \binom{n-1} j p^{j}q^{(n-1)-j}.
    \end{align*} Далее,
    \begin{align*}
     \E\xi^{2} &= \sum_{k=0}^{n}k^{2} \cdot \binom n k p^{k}q^{n-k} = \sum_{k=1}^{n} k(k-1) \cdot \binom n k p^{k}q^{n-k} + \sum_{k=1}^{n} k \cdot \binom n k p^{k}q^{n-k} = \\
     &= \sum_{k=2}^{n} \frac{n!}{(k-2)!(n-k)!} p^{k}q^{n-k} + \E\xi = n(n-1)p^{2} \sum_{k=2}^{n} \binom{n-2}{k-2} p^{k-2}q^{n - k} + np = \\
     &= n(n-1)p^{2} \sum_{j=0}^{n-2}\binom{n-2}{j} p^{j} q^{(n-2)-j} + np = n(n-1)p^{2} + np.
    \end{align*} Тогда
    \begin{align*}
     \D\xi = \E\xi^{2} - (\E\xi)^{2} = n(n-1)p^{2} + np - n^{2}p^{2} = np - np^{2} = npq.
    \end{align*} Итого,
    \begin{align*}
     \E\xi = np, && \D\xi = npq.
    \end{align*}
   \end{exmpl}

   \begin{exmpl}[моменты распределения Пуассона]
    Пусть $ \xi\sim\Poisson(\lambda) $,  $ \lambda > 0 $. Тогда
    \begin{align*}
     \E\xi =e^{-\lambda}  \sum_{k=0}^{\infty} \frac{k\lambda^{k}}{k!} = \lambda \cdot e^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1)!} e^{-\lambda} = \lambda \cdot e^{\lambda}e^{-\lambda} = \lambda.
    \end{align*} Далее,
    \begin{align*}
     \E\xi^{2} &= e^{-\lambda}\sum_{k=0}^{\infty}\frac{k^{2}\lambda^{k}}{k!} = e^{-\lambda} \sum_{k=2}^{\infty} \frac{k(k-1)\lambda^{k}}{k!} + \E\xi = \lambda^{2} \cdot e^{-\lambda} \sum_{k=2}^{\infty} \frac{\lambda^{k-2}}{(k-2)!} + \lambda = \lambda^{2}+\lambda.
    \end{align*} Тогда
    \begin{align*}
     \D\xi = \E\xi^{2} - (\E\xi)^{2} = \lambda^{2} + \lambda - \lambda^{2} = \lambda.
    \end{align*} Итого,
    \begin{align*}
     \E\xi = \lambda, && \D\xi = \lambda.
    \end{align*} Как матожидание, так и дисперсия распределения Пуассона равны его параметру $ \lambda $.
   \end{exmpl} 

   \begin{exmpl}[моменты геометрического распределения]
    Пусть $ \xi\sim\Geom(p) $. Тогда
    \begin{align*}
     \E\xi &= p \sum_{k=1}^{\infty} k q^{k - 1} = p \left( \sum_{k=0}^{\infty} x^{k} \right)'_x(q) = p \left( \frac{1}{1-x} \right)'_x(q) = \\
     &= p \left( \frac{1}{(1-x)^{2}} \right)(q) = p \cdot \frac{1}{(1-q)^{2}} = \frac{1}{p}.
    \end{align*} Таким образом, если вероятность успеха равна $ p $, то ожидаемый номер первого успеха равен $ 1 / p $. Далее,
    \begin{align*}
     \E\xi^{2} &= p \sum_{k=1}^{\infty}k^{2}q^{k-1} = pq\sum_{k=2}^{\infty}k(k-1)q^{k-2} + \E\xi = pq \left( \sum_{k=0}^{\infty}x^{k} \right)''_x(q) + \frac{1}{p} = \\
     &= pq \left( \frac{1}{1-x} \right)''_x(q) + \frac{1}{p} = pq \left( \frac{1}{(1-x)^{2}} \right)'_x(q) + \frac{1}{p} = pq \left( \frac{2}{(1-x)^{3}} \right)(q)+\frac{1}{p} = \\
     &= \frac{2q}{p^{2}} + \frac{1}{p} = \frac{2 - p}{p^{2}}.
    \end{align*} Тогда
    \begin{align*}
     \D\xi = \E\xi^{2} - (\E\xi)^{2} = \frac{2-p}{p^{2}} - \frac{1}{p^{2}} = \frac{q}{p^{2}}.
    \end{align*} Итого,
    \begin{align*}
     \E\xi = \frac{1}{p}, && \D\xi = \frac{q}{p^{2}}.
    \end{align*}
   \end{exmpl}

   \begin{exmpl}[моменты дискретного равномерного распределения]
    Пусть $ \xi $ равномерно распределена на  $ \left\{ 1,2,\ldots,n \right\} $. Тогда
    \begin{align*}
     \E\xi = \frac{1}{n} \sum_{k=1}^{n} k = \frac{1}{n} \cdot \frac{n(n+1)}{2} = \frac{n+1}{2}.
    \end{align*} Далее,
    \begin{align*}
     \E\xi^{2} = \frac{1}{n} \sum_{k=1}^{n}k^{2} = \frac{1}{n}\cdot \frac{n(n+1)(2n+1)}{6} = \frac{(n+1)(2n+1)}{6}.
    \end{align*} Тогда
    \begin{align*}
     \D\xi &= \E\xi^{2} - (\E\xi)^{2} = \frac{(n+1)(2n+1)}{6} - \frac{(n+1)^{2}}{4} = \frac{2(n+1)(2n+1) - 3(n+1)^{2}}{12} = \\
     &= \frac{(n+1)(2(2n+1) - 3(n+1))}{12} = \frac{(n+1)(n-1)}{12} = \frac{n^{2}-1}{12}.
    \end{align*} Итого,
    \begin{align*}
     \E\xi = \frac{n+1}{2}, && \D\xi = \frac{n^{2}-1}{12}.
    \end{align*}
   \end{exmpl}

   \begin{exmpl}[моменты стандартного экспоненциального распределения]
    Пусть $ \xi\sim\Exp(1) $ --- имеет стандартное экспоненциальное распределение,  $ p_\xi(t) = e^{-t} $, $ t \geqslant 0 $ --- его плотность. Тогда
    \begin{align*}
     \E\xi = \int_{0}^{\infty} xe^{-x}\,dx = \left.(-xe^{-x})\right|_0^{\infty} - \int_{0}^{\infty} (-e^{-x})\,dx = \int_{0}^{\infty} e^{-x}\,dx = \left.(-e^{-x})\right|_0^{\infty} = 1.
      \end{align*} Далее,
      \begin{align*}
       \E\xi^{2} = \int_{0}^{\infty} x^{2}e^{-x}\,dx = \left.(x^{2}e^{-x})\right|_0^{\infty} - \int_{0}^{\infty} (-2xe^{-x})\,dx = 2 \int_{0}^{\infty} xe^{-x}\,dx = 2.
       \end{align*} Тогда,
       \begin{align*}
        \D\xi = \E\xi^{2} - (\E\xi)^{2} = 2 - 1 = 1.
       \end{align*}
      \end{exmpl}
      \begin{exmpl}[моменты экспоненциального распределения]
       Пусть $ \xi\sim\Exp(\lambda) $ --- имеет экспоненциальное распределение с параметром $ \lambda > 0 $, его плотность равна
       \begin{align*}
        p_\xi(t) = \lambda e^{-\lambda t}, \qquad t \geqslant 0.
       \end{align*} Тогда
       \begin{align*}
        \xi = \frac{\eta}{\lambda},
       \end{align*} где $ \eta \sim \Exp(1) $. Действительно, по арифметическим свойствам плотности
       \begin{align*}
        p_\eta(t) = p_{\lambda \xi}(t) = \frac{1}{\lambda} \cdot  p_\xi(t / \lambda) = \frac{1}{\lambda} \cdot \lambda e^{-\lambda \cdot t / \lambda} = e^{-t}, \qquad t \geqslant 0.
       \end{align*} В таком случае
       \begin{align*}
        \E\xi = \frac{\E\eta}{\lambda} = \frac{1}{\lambda}, && \D\xi = \frac{\D\eta}{\lambda^{2}} = \frac{1}{\lambda^{2}}.
       \end{align*}
      \end{exmpl}

      \subsection{Ковариация.}

      \begin{df}[ковариация]
       Пусть случайные величины $ \xi $ и $ \eta $ имеют математическое ожидание. Тогда \textit{ковариацией} $ \xi $ и $ \eta $ называется число
       \begin{align*}
        \cov(\xi,\eta) = \E \left( (\xi - \E\xi)(\eta - \E\eta) \right),
       \end{align*} если, конечно, оно существует.
      \end{df}

      \begin{prop}[существование ковариации]
       Пусть случайные величины $ \xi $ и $ \eta $ имеют конечный второй момент: $ \E\xi^{2} < \infty $ и $ \E\eta^{2} < \infty $. Тогда их ковариация $ \cov(\xi,\eta) $ определена и конечна.
      \end{prop}
      \begin{proof}[\normalfont\textsc{Доказательство}]
       Математическое ожидание существует, так как из существования конечного второго момента следует существование всевозможных первых и вторых моментов. Воспользуемся неравенством Гёльдера \eqref{equation:holder_inequality} для чисел $ p = q = 2 $:
       \begin{align*}
        \E \left| (\xi - \E\xi)(\eta-\E\eta) \right| \leqslant \left( \E \left| \xi-\E\xi \right|^{2} \right)^{1 / 2} \left( \E \left| \eta-\E\eta \right|^{2} \right)^{1 / 2} < \infty.
       \end{align*}
      \end{proof}

      \begin{prop}[свойства ковариации]\
       \begin{enumerate}
        \item $ \cov(\xi,\eta) = \E(\xi\eta) - \E\xi \cdot \E\eta $.
        \item Коммутативность: $ \cov(\xi,\eta)=\cov(\eta,\xi) $.
        \item Линейность:
         \begin{align*}
          \cov(a\xi_1 + b\xi_2, \eta) = a \cdot \cov(\xi_1,\eta) + b \cdot \cov(\xi_2,\eta).
         \end{align*}
       \end{enumerate}
      \end{prop}
      \begin{proof}[\normalfont\textsc{Доказательство}]\
       \begin{enumerate}
        \item Действительно,
         \begin{align*}
          \cov(\xi,\eta) &= \E ((\xi-\E\xi)(\eta-\E\eta)) = \E(\xi\eta - \xi \cdot \E\eta - \E\xi \cdot \eta + \E\xi \cdot \E\eta) \\
          &= \E(\xi\eta) - \E\xi \cdot \E\eta - \E\xi\cdot\E\eta + \E\xi\cdot\E\eta = \\
          &= \E(\xi\eta)-\E\xi\cdot\E\eta.
         \end{align*}
        \item Очевидно из определения.
        \item Легко вывести из линейности:
         \begin{align*}
          \cov(a\xi_1+b\xi_2,\eta) &= \E((a\xi_1+b\xi_2)\eta) - \E(a\xi_1+b\xi_2) \cdot \E\eta = \\
          &= a\E(\xi_1\eta) + b\E(\xi_2\eta) - a\E\xi_1 \cdot \E\eta - b \E\xi_2 \cdot \E\eta = \\
          &= a \cdot \cov(\xi_1,\eta) + b \cdot \cov(\xi_2,\eta).
         \end{align*}
       \end{enumerate}
      \end{proof}

      Посмотрим, как связана ковариация и дисперсия.

      \begin{prop}
       $\cov(\xi,\xi) = \D\xi$.
      \end{prop}
      \begin{proof}[\normalfont\textsc{Доказательство}]
       Действительно,
       \begin{align*}
        \cov(\xi,\xi)=\E((\xi-\E\xi)(\xi-\E\xi))=\E(\xi-\E\xi)^{2}=\D\xi.
       \end{align*}
      \end{proof}

      \begin{prop}[дисперсия суммы]
       \begin{align}
        \label{eq:variance_sum}
        \D(\xi + \eta) = \D\xi + \D\eta + 2\cov(\xi,\eta).
       \end{align}
      \end{prop}
      \begin{proof}[\normalfont\textsc{Доказательство}]
       \begin{align*}
        \D(\xi+\eta) &= \E(\xi+\eta)^{2} - \left( \E(\xi+\eta) \right)^{2} = \\
        &= \E\xi^{2} + \E\eta^{2} + 2\E(\xi\eta) - (\E\xi)^{2} - (\E\eta)^{2} - 2\E\xi \cdot \E\eta = \\
        &= \D\xi + \D\eta +2\cov(\xi,\eta).
       \end{align*} 
      \end{proof}

      Равенство \eqref{eq:variance_sum} для дисперсии суммы легко обобщается для суммы $ n $ случайных величин.

      \begin{prop}
       \begin{align}
        \label{eq:variance_sum_many}
        \D(\xi_1 + \xi_2 + \ldots + \xi_n) = \sum_{i=1}^{n} \D\xi_i + 2 \sum_{i < j} \cov(\xi_i, \xi_j).
       \end{align} 
      \end{prop}
      \begin{proof}[\normalfont\textsc{Доказательство}]
       Нужно много раз применить \eqref{eq:variance_sum} и линейность ковариации.
      \end{proof}

      \begin{df}[некоррелированные случайные величины]
       Случайные величины $ \xi $ и $ \eta $ называются \textit{некоррелированными}, если $ \cov(\xi,\eta)=0 $.
      \end{df}

      Посмотрим, как связаны некоррелированность и независимость.

      \begin{prop}
       Если cлучайные величины $ \xi $ и $ \eta $ независимы, то они некоррелированые: $\cov(\xi,\eta) = 0$. 
      \end{prop}
      \begin{proof}[\normalfont\textsc{Доказательство}]
       \begin{align*}
        \cov(\xi,\eta) = \E(\xi\eta) - \E\xi \cdot \E\eta = 0
       \end{align*} по формуле \eqref{equation:independednt_expected_product}.
      \end{proof}

      \begin{exmpl}
       Из некоррелированности не следует независимость!

       Рассмотрим дискретное пространство элементарных исходов с тремя равновероятными исходами: $ \Omega = \left\{ \omega_1, \omega_2,\omega_3 \right\}$. Рассмотрим следующие случайные величины:
       \begin{center}
        \begin{tabular}{r|ccc}
         & $\omega_1$ & $\omega_2$ & $\omega_3$ \\
         \hline
         $\xi$ & $ 1 $ & $-1$ & $0$  \\
         $\eta$ & $0$ & $0$ & $1$ 
        \end{tabular}
       \end{center} Так как $ \E\xi = \frac{1 + (-1) + 0}{3} = 0 $, то $ \E\xi \cdot \E\eta = 0 $. Кроме того, $ \E(\xi\eta) = 0 $, значит $ \cov(\xi,\eta)=0 $. Но независимости нет!
      \end{exmpl}

      Масштаб ковариации зависит от разброса случайных величин, поэтому по абсолютному значению ковариации нельзя судить о том, насколько величины взаимосвязаны. Однако, для такой цели подходит другая характеристика.

      \begin{df}[коэффициент корреляции]
       Пусть случайные величины $ \xi $ и $ \eta $ имеют ненулевую дисперсию. Тогда \textit{коэффициентом корреляции} $ \xi $ и $ \eta $ называется число
       \begin{align*}
        \rho(\xi,\eta) = \frac{\cov(\xi,\eta)}{\sqrt{\D\xi} \cdot \sqrt{\D\eta}}.
       \end{align*}
      \end{df}

      \begin{prop}
       $ -1 \leqslant \rho(\xi,\eta) \leqslant 1 $.
      \end{prop}
      \begin{proof}[\normalfont\textsc{Доказательство}]
       По неравенству Гёльдера \eqref{equation:holder_inequality}
       \begin{align*}
        \left| \cov(\xi,\eta) \right| \leqslant \E \left| (\xi-\E\xi)(\eta-\E\eta) \right| \leqslant \left( \E \left| \xi-\E\xi \right|^{2} \right)^{1 / 2} \left( \E \left| \eta-\E\eta \right|^{2} \right)^{1 / 2} = \sqrt{\D\xi} \cdot \sqrt{\D\eta}.
       \end{align*}
      \end{proof}

      \begin{exercs*}
       Пусть $ \rho(\xi,\eta) = \pm 1 $. Доказать, что тогда $ \xi = a\eta + b $, где $ a, b \in \R $ и $ \mathrm{sign}\;a = \mathrm{sign}\;\rho(\xi,\eta) $.
      \end{exercs*}

      \end{document}
